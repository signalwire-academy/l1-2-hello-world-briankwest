name: Grade Submission

on:
  push:
    branches: [main]
    paths:
      - 'solution/**'
  workflow_dispatch:

permissions:
  contents: read
  issues: write
  checks: write

env:
  PYTHON_VERSION: '3.11'

jobs:
  grade:
    name: Auto-Grade
    runs-on: ubuntu-latest
    timeout-minutes: 5

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          pip install -q signalwire-agents pyyaml

      - name: Check solution exists
        id: check
        run: |
          if [ -f "solution/agent.py" ]; then
            # Check if file has actual code (not just placeholder)
            if grep -q "AgentBase" solution/agent.py; then
              echo "exists=true" >> $GITHUB_OUTPUT
            else
              echo "exists=false" >> $GITHUB_OUTPUT
              echo "::warning::solution/agent.py does not contain AgentBase"
            fi
          else
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "::warning::No solution/agent.py found"
          fi

      - name: Run grading
        if: steps.check.outputs.exists == 'true'
        id: grade
        run: |
          python << 'EOF'
          import subprocess
          import json
          import yaml
          import sys
          import os

          # Load test config
          with open('tests/grading.yaml') as f:
              config = yaml.safe_load(f)

          results = {
              'assignment': config['assignment'],
              'checks': [],
              'score': 0,
              'max_score': 0,
              'passed': False
          }

          agent_file = 'solution/agent.py'

          for check in config['checks']:
              check_result = {
                  'id': check['id'],
                  'name': check['name'],
                  'max_points': check['points'],
                  'points': 0,
                  'passed': False,
                  'output': ''
              }

              try:
                  if check['type'] == 'instantiate':
                      # Try to load the agent
                      result = subprocess.run(
                          ['swaig-test', agent_file, '--list-tools'],
                          capture_output=True, text=True, timeout=30
                      )
                      check_result['passed'] = result.returncode == 0
                      if result.returncode != 0:
                          check_result['output'] = result.stderr[:200]

                  elif check['type'] == 'swml_valid':
                      result = subprocess.run(
                          ['swaig-test', agent_file, '--dump-swml', '--raw'],
                          capture_output=True, text=True, timeout=30
                      )
                      if result.returncode == 0:
                          try:
                              swml = json.loads(result.stdout)
                              check_result['passed'] = True

                              # Check required paths
                              for req in check.get('require', []):
                                  path = req['path']
                                  contains = req.get('contains')

                                  # Navigate path
                                  value = swml
                                  for part in path.replace('[', '.').replace(']', '').split('.'):
                                      if part.isdigit():
                                          value = value[int(part)]
                                      else:
                                          value = value.get(part)
                                      if value is None:
                                          check_result['passed'] = False
                                          check_result['output'] = f"Missing: {path}"
                                          break

                                  if check_result['passed'] and contains:
                                      if contains.lower() not in str(value).lower():
                                          check_result['passed'] = False
                                          check_result['output'] = f"'{contains}' not found in {path}"

                          except json.JSONDecodeError:
                              check_result['output'] = 'Invalid JSON output'
                              check_result['passed'] = False
                      else:
                          check_result['output'] = result.stderr[:200]

              except subprocess.TimeoutExpired:
                  check_result['output'] = 'Timeout exceeded'
              except Exception as e:
                  check_result['output'] = str(e)[:200]

              if check_result['passed']:
                  check_result['points'] = check['points']

              results['checks'].append(check_result)

          # Calculate totals
          results['max_score'] = sum(c['max_points'] for c in results['checks'])
          results['score'] = sum(c['points'] for c in results['checks'])
          results['percentage'] = round(results['score'] / results['max_score'] * 100, 1) if results['max_score'] > 0 else 0
          results['passed'] = results['percentage'] >= config['assignment']['passing_score']

          # Save results
          with open('results.json', 'w') as f:
              json.dump(results, f, indent=2)

          # Set outputs
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"score={results['score']}\n")
              f.write(f"max_score={results['max_score']}\n")
              f.write(f"percentage={results['percentage']}\n")
              f.write(f"passed={str(results['passed']).lower()}\n")

          print(f"Score: {results['score']}/{results['max_score']} ({results['percentage']}%)")
          print(f"Status: {'PASSED' if results['passed'] else 'NOT PASSING'}")
          EOF

      - name: Generate report
        if: steps.check.outputs.exists == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const yaml = require('js-yaml');

            let results;
            try {
              results = JSON.parse(fs.readFileSync('results.json', 'utf8'));
            } catch (e) {
              console.log('No results file found');
              return;
            }

            const config = yaml.load(fs.readFileSync('tests/grading.yaml', 'utf8'));

            // Build markdown report
            let report = `## Grading Results\n\n`;
            report += `**Assignment:** ${results.assignment.name}\n`;
            report += `**Score:** ${results.score}/${results.max_score} (${results.percentage}%)\n`;
            report += `**Status:** ${results.passed ? 'PASSED' : 'NOT PASSING'}\n\n`;

            report += `### Checks\n\n`;
            report += `| # | Check | Points | Status |\n`;
            report += `|---|-------|--------|--------|\n`;

            results.checks.forEach((check, i) => {
              const status = check.passed ? '[x]' : '[ ]';
              report += `| ${i+1} | ${check.name} | ${check.points}/${check.max_points} | ${status} |\n`;
            });

            // Add feedback for failed checks
            const failed = results.checks.filter(c => !c.passed && c.output);
            if (failed.length > 0) {
              report += `\n### Details\n\n`;
              failed.forEach(check => {
                report += `**${check.name}:** ${check.output}\n\n`;
              });
            }

            // Add pass/fail message
            report += `\n---\n\n`;
            report += results.passed ? config.feedback.pass : config.feedback.fail;

            report += `\n\n---\n*Graded: ${new Date().toISOString()}*`;

            // Find or create results issue
            const { data: issues } = await github.rest.issues.listForRepo({
              owner: context.repo.owner,
              repo: context.repo.repo,
              labels: 'grading',
              state: 'open'
            });

            if (issues.length > 0) {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: issues[0].number,
                body: report
              });
            } else {
              await github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: 'Grading Results',
                body: report,
                labels: ['grading']
              });
            }

      - name: Upload results
        if: steps.check.outputs.exists == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: grading-results
          path: results.json
          retention-days: 90
